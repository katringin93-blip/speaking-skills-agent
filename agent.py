import sys
import time
import shutil
import subprocess
import json
from pathlib import Path
from typing import List, Dict, Tuple

import yaml
import requests
from pydub import AudioSegment

# ---------- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ----------
CHECK_INTERVAL_SECONDS = 2
CHUNK_LENGTH_MS = 1200 * 1000  # 20 –º–∏–Ω—É—Ç (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –ª–∏–º–∏—Ç–∞ OpenAI –≤ 1400 —Å–µ–∫)

# ---------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------

def pause_and_exit(code: int = 0):
    print("\n---")
    print("–ù–∞–∂–º–∏—Ç–µ ENTER, —á—Ç–æ–±—ã –∑–∞–∫—Ä—ã—Ç—å –æ–∫–Ω–æ...")
    try:
        input()
    except Exception:
        pass
    sys.exit(code)

def load_config() -> dict:
    cfg_path = Path(__file__).resolve().parent / "config.local.yaml"
    if not cfg_path.exists():
        raise FileNotFoundError(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {cfg_path}")
    return yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}

def extract_audio(ffmpeg: Path, input_video: Path, output_audio: Path):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–≤—É–∫ –≤ MP3 —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º —Å–∂–∞—Ç–∏–µ–º –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
    output_path = output_audio.with_suffix('.mp3')
    cmd = [
        str(ffmpeg), "-y",
        "-i", str(input_video),
        "-vn", "-ac", "1", "-ar", "16000", "-b:a", "48k",
        str(output_path)
    ]
    subprocess.run(cmd, capture_output=True)
    return output_path

def slice_audio(audio_path: Path) -> List[Path]:
    """–†–∞–∑—Ä–µ–∑–∞–µ—Ç –∞—É–¥–∏–æ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ 20 –º–∏–Ω—É—Ç, —á—Ç–æ–±—ã –æ–±–æ–π—Ç–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è API"""
    audio = AudioSegment.from_file(audio_path)
    chunks = []
    for i, chunk in enumerate(audio[::CHUNK_LENGTH_MS]):
        chunk_p = audio_path.parent / f"chunk_{i}.mp3"
        chunk.export(chunk_p, format="mp3")
        chunks.append(chunk_p)
    return chunks

# ---------- –†–∞–±–æ—Ç–∞ —Å OpenAI API ----------

def transcribe_chunk(api_key: str, chunk_path: Path) -> dict:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∞—É–¥–∏–æ-—Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é"""
    url = "https://api.openai.com/v1/audio/transcriptions"
    headers = {"Authorization": f"Bearer {api_key}"}
    data = {
        "model": "gpt-4o-transcribe-diarize",
        "response_format": "diarized_json",
        "chunking_strategy": json.dumps({"type": "server_vad"})
    }
    
    with chunk_path.open("rb") as f:
        files = {"file": (chunk_path.name, f, "audio/mpeg")}
        # –î–ª–∏—Ç–µ–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—è–∂–µ–ª—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        r = requests.post(url, headers=headers, files=files, data=data, timeout=900)
    
    if r.status_code != 200:
        raise RuntimeError(f"OpenAI Transcription Error {r.status_code}: {r.text}")
    return r.json()

def analyze_speech_contextual(api_key: str, full_transcript_path: Path, me_id: str) -> str:
    """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π —Ä–µ—á–∏ —Å —É—á–µ—Ç–æ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞–º–∏"""
    if not full_transcript_path.exists():
        return "–û—à–∏–±–∫–∞: —Ñ–∞–π–ª —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω."
    
    full_text = full_transcript_path.read_text(encoding="utf-8")
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞ –¥–ª—è GPT (–æ–∫–æ–ª–æ 25–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
    clipped_text = full_text[-25000:] if len(full_text) > 25000 else full_text
    
    current_date = time.strftime("%Y-%m-%d %H:%M:%S")
    header = f"=== SESSION ANALYSIS REPORT ===\nDate: {current_date}\nUser ID: {me_id}\n\n"

    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    
    prompt = f"""
    You are a professional English Language Examiner (IELTS/Cambridge expert). 
    Analyze the speech of the PRIMARY SPEAKER (marked as 'YOU') in the context of the entire conversation.
    
    FULL TRANSCRIPT FOR CONTEXT:
    {clipped_text}
    
    REPORT STRUCTURE (Please provide responses in Russian):
    1. **Fluency & Coherence**: (–¢–µ–º–ø —Ä–µ—á–∏, –ø–ª–∞–≤–Ω–æ—Å—Ç—å, –Ω–∞–ª–∏—á–∏–µ –ø–∞—É–∑ –∏ —Å–∞–º–æ–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π)
    2. **Grammatical Range & Accuracy**: (–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Ç–æ—á–Ω–æ—Å—Ç—å –∏—Ö –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
    3. **Lexical Resource**: (–°–ª–æ–≤–∞—Ä–Ω—ã–π –∑–∞–ø–∞—Å, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–¥–∏–æ–º, —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–¥–±–æ—Ä–∞ —Å–ª–æ–≤)
    4. **Pronunciation & Dynamic**: (–†–∏—Ç–º, –∏–Ω—Ç–æ–Ω–∞—Ü–∏—è –∏ –¥–∏–Ω–∞–º–∏–∫–∞ —Ä–µ—á–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤)
    5. **Discourse & Interactional Competence**: (–û—Ü–µ–Ω–∏, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–ø–∏–∫–µ—Ä —Ä–µ–∞–≥–∏—Ä—É–µ—Ç –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏ –∏ –≤–æ–ø—Ä–æ—Å—ã —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–≤. –ù–∞—Å–∫–æ–ª—å–∫–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤–µ–¥–µ—Ç—Å—è –¥–∏–∞–ª–æ–≥.)
    
    FINAL ASSESSMENT:
    - Estimated CEFR Level (A1-C2):
    - Key advice for the next session: (–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–æ–≤–µ—Ç –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è)
    """

    payload = {
        "model": "gpt-4o",
        "messages": [
            {"role": "system", "content": "You are an expert linguist analyzing conversational interaction in English."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3
    }

    try:
        r = requests.post(url, headers=headers, json=payload, timeout=120)
        if r.status_code == 200:
            return header + r.json()['choices'][0]['message']['content']
        else:
            return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ GPT: {r.status_code}\n{r.text}"
    except Exception as e:
        return f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}"

# ---------- –ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –ø—Ä–æ–≥—Ä–∞–º–º—ã ----------

def main():
    print(">>> Agent started. Monitoring for new recordings...")
    config = load_config()
    
    obs_dir = Path(config["paths"]["obs_recordings_dir"])
    sess_root = Path(config["paths"]["sessions_dir"])
    api_key = config["whisper_api"]["api_key"]
    ffmpeg_exe = Path(config["paths"]["ffmpeg_path"])

    processed_files = set()

    while True:
        if not obs_dir.exists():
            time.sleep(5)
            continue

        for video_file in obs_dir.iterdir():
            if video_file.suffix.lower() in (".mp4", ".mkv") and video_file not in processed_files:
                # –î–∞–µ–º –≤—Ä–µ–º—è —Ñ–∞–π–ª—É "–¥–æ–∑–∞–ø–∏—Å–∞—Ç—å—Å—è" –∏ –∑–∞–∫—Ä—ã—Ç—å—Å—è —Å–∏—Å—Ç–µ–º–æ–π
                time.sleep(5)
                
                print(f"\n[NEW] –ù–∞–π–¥–µ–Ω–∞ –∑–∞–ø–∏—Å—å: {video_file.name}")
                session_id = time.strftime("%Y-%m-%d_%H-%M-%S")
                session_dir = sess_root / session_id
                session_dir.mkdir(parents=True, exist_ok=True)
                
                try:
                    # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–≤—É–∫–∞
                    print("[1/4] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏...")
                    full_audio_mp3 = extract_audio(ffmpeg_exe, video_file, session_dir / "full_audio")
                    
                    # 2. –ù–∞—Ä–µ–∑–∫–∞ –Ω–∞ —á–∞—Å—Ç–∏
                    print("[2/4] –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ö–æ–¥–∞ –ª–∏–º–∏—Ç–æ–≤...")
                    audio_chunks = slice_audio(full_audio_mp3)
                    
                    all_segments = []
                    current_time_offset = 0.0
                    
                    # 3. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –ø–æ —á–∞—Å—Ç—è–º
                    for i, chunk_p in enumerate(audio_chunks):
                        print(f"    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ {i+1} –∏–∑ {len(audio_chunks)}...")
                        chunk_data = transcribe_chunk(api_key, chunk_p)
                        
                        for seg in chunk_data.get("segments", []):
                            seg["start"] += current_time_offset
                            seg["end"] += current_time_offset
                            # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –≥–ª–æ–±–∞–ª—å–Ω—ã–π ID —Å–ø–∏–∫–µ—Ä—É
                            seg["global_id"] = f"P{i}_{seg.get('speaker', 'UNK')}"
                            all_segments.append(seg)
                        
                        current_time_offset += (len(AudioSegment.from_file(chunk_p)) / 1000.0)
                        chunk_p.unlink() # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª

                    # --- –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –û–°–ù–û–í–ù–û–ì–û –°–ü–ò–ö–ï–†–ê (–í–ê–°) ---
                    speaker_durations = {}
                    for s in all_segments:
                        gid = s["global_id"]
                        dur = s["end"] - s["start"]
                        speaker_durations[gid] = speaker_durations.get(gid, 0) + dur
                    
                    if not speaker_durations:
                        print("!!! –†–µ—á—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞.")
                        continue

                    # –í—ã ‚Äî —Ç–æ—Ç, –∫—Ç–æ –Ω–∞–≥–æ–≤–æ—Ä–∏–ª –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ —Å–µ–∫—É–Ω–¥ –∑–∞ –≤—Å—é —Å—É–º–º–∞—Ä–Ω—É—é —Å–µ—Å—Å–∏—é
                    main_speaker_id = max(speaker_durations, key=speaker_durations.get)
                    print(f"[ID] –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–∏–∫–µ—Ä –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫: {main_speaker_id}")

                    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ê–Ω–∞–ª–∏–∑
                    print("[4/4] –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∏ –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")
                    final_lines = []
                    for s in all_segments:
                        label = "YOU" if s["global_id"] == main_speaker_id else s["global_id"]
                        final_lines.append(f"[{s['start']:.1f}-{s['end']:.1f}] {label}: {s.get('text','')}")
                    
                    transcript_file = session_dir / "transcript_full.txt"
                    transcript_file.write_text("\n".join(final_lines), encoding="utf-8")
                    
                    # –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ AI-–∞–Ω–∞–ª–∏–∑–∞
                    analysis_report = analyze_speech_contextual(api_key, transcript_file, main_speaker_id)
                    (session_dir / "ai_analysis_report.txt").write_text(analysis_report, encoding="utf-8")
                    
                    print("\n" + "="*50)
                    print(analysis_report)
                    print("="*50)
                    print(f"[SUCCESS] –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ø–∞–ø–∫—É: {session_id}")
                    
                    # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ –∫–æ–Ω—Ñ–∏–≥–µ)
                    tg_conf = config.get("telegram", {})
                    if tg_conf.get("enabled"):
                        msg = f"üìä *New English Session Analysis*\n\n{analysis_report[:3800]}"
                        requests.post(f"https://api.telegram.org/bot{tg_conf['bot_token']}/sendMessage", 
                                      json={"chat_id": tg_conf['chat_id'], "text": msg, "parse_mode": "Markdown"})

                    processed_files.add(video_file)

                except Exception as e:
                    print(f"!!! –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {e}")

        time.sleep(CHECK_INTERVAL_SECONDS)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n–ü—Ä–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(0)
    except Exception as e:
        print(f"\n–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        pause_and_exit(1)
